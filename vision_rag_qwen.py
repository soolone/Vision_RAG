import os
import io
import base64
import PIL
from PIL import Image
import numpy as np
import streamlit as st
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Import all agent tools
try:
    from agent.data_agent import DataAgent
    from agent.chunk_agent import ChunkAgent, ChunkConfig
    from agent.chromadb_agent import ChromaDBManager
    from agent.qwen_embedding import TongyiEmbedding
    from agent.query_agent import QueryExpansionAgent
    from agent.answer_agent import AnswerAgent
except ImportError as e:
    st.error(f"å¯¼å…¥é”™è¯¯: {e}")
    DataAgent = None
    ChunkAgent = None
    ChunkConfig = None
    ChromaDBManager = None
    TongyiEmbedding = None
    QueryExpansionAgent = None
    AnswerAgent = None

# --- Helper functions ---

def process_document_with_agents(file_path: str, config) -> List:
    """ä½¿ç”¨å·¥å…·ç±»å¤„ç†æ–‡æ¡£"""
    try:
        # åˆå§‹åŒ–æ•°æ®ä»£ç†
        data_agent = DataAgent(
            output_dir=config.output_dir,
            cls_dir=config.cls_dir,
            lang=config.lang,
            enable_formula=config.enable_formula,
            enable_table=config.enable_table,
            auto_caption=config.auto_caption
        )
        
        # è§£ææ–‡æ¡£
        parse_result = data_agent.parse_document(file_path)
        
        if not parse_result.success:
            st.error(f"æ–‡æ¡£è§£æå¤±è´¥: {parse_result.error_message}")
            return []
        
        # åˆå§‹åŒ–åˆ‡ç‰‡ä»£ç†
        chunk_config = ChunkConfig(
            max_chunk_size=config.max_chunk_size,
            min_chunk_size=config.min_chunk_size,
            overlap_size=config.overlap_size,
            preserve_sentences=config.preserve_sentences,
            preserve_paragraphs=config.preserve_paragraphs
        )
        chunk_agent = ChunkAgent(chunk_config)
        
        # æŸ¥æ‰¾content_listæ–‡ä»¶
        content_list_files = list(Path(parse_result.output_dir).rglob("*_content_list.json"))
        if not content_list_files:
            st.error("æœªæ‰¾åˆ°content_listæ–‡ä»¶")
            return []
        
        # å¯¹æ–‡æ¡£è¿›è¡Œåˆ‡ç‰‡
        # ä½¿ç”¨content_listæ–‡ä»¶çš„çˆ¶ç›®å½•ä½œä¸ºoutput_dirï¼Œè¿™æ ·å¯ä»¥æ­£ç¡®æ‰¾åˆ°imagesç›®å½•
        actual_output_dir = str(content_list_files[0].parent)
        chunks = chunk_agent.chunk_document(
            str(content_list_files[0]),
            actual_output_dir,
            Path(file_path).stem
        )
        
        return chunks
        
    except Exception as e:
        st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        return []

def display_knowledge_base_management():
    """æ˜¾ç¤ºçŸ¥è¯†åº“ç®¡ç†ç•Œé¢"""
    st.header("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
    
    # è·å–æ•°æ®åº“ç®¡ç†å™¨
    if 'db_manager' not in st.session_state or st.session_state.db_manager is None:
        st.warning("æ•°æ®åº“ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½®æ•°æ®åº“ã€‚")
        return
    
    db_manager = st.session_state.db_manager
    
    # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å’Œæ‰€æœ‰æ–‡æ¡£
    try:
        collection_info = db_manager.get_collection_info()
        total_chunks = collection_info.get('count', 0)
        
        if total_chunks == 0:
            st.info("çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ å¹¶è§£ææ–‡æ¡£ã€‚")
            return
        
        # æŸ¥è¯¢æ‰€æœ‰æ–‡æ¡£
        all_docs = db_manager.collection.get(
            include=['metadatas', 'documents']
        )
        
        if not all_docs['metadatas']:
            st.info("çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£ã€‚")
            return
        
        # è®¡ç®—æ–‡æ¡£æ•°é‡
        unique_docs = set()
        for metadata in all_docs['metadatas']:
            doc_name = metadata.get('parent_document') or metadata.get('source_file', 'unknown')
            if doc_name != 'unknown':
                unique_docs.add(doc_name)
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("æ–‡æ¡£æ•°é‡", len(unique_docs))
        with col2:
            st.metric("æ€»åˆ‡ç‰‡æ•°é‡", total_chunks)
            
    except Exception as e:
        st.error(f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
        return
    
    # æŒ‰æ–‡æ¡£åˆ†ç»„
    documents_dict = {}
    for i, metadata in enumerate(all_docs['metadatas']):
        # ä¼˜å…ˆä½¿ç”¨parent_documentï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨source_file
        doc_name = metadata.get('parent_document') or metadata.get('source_file', 'unknown')
        if doc_name not in documents_dict:
            documents_dict[doc_name] = []
        
        chunk_data = {
            'chunk_id': metadata.get('chunk_id', f'chunk_{i}'),
            'chunk_type': metadata.get('chunk_type', 'text'),
            'page_idx': metadata.get('page_idx') or metadata.get('page_number', 0),
            'chunk_idx': metadata.get('chunk_idx', 0),
            'content': all_docs['documents'][i],
            'metadata': metadata
        }
        documents_dict[doc_name].append(chunk_data)
    
    # æ˜¾ç¤ºæ–‡æ¡£åˆ—è¡¨
    st.subheader("ğŸ“„ å·²è§£ææ–‡æ¡£")
    
    for doc_name, chunks in documents_dict.items():
            # æŒ‰é¡µé¢å’Œåˆ‡ç‰‡ç´¢å¼•æ’åº
            chunks.sort(key=lambda x: (x['page_idx'], x['chunk_idx']))
            
            # æ˜¾ç¤ºæ–‡æ¡£ç»Ÿè®¡
            chunk_types = {}
            for chunk in chunks:
                chunk_type = chunk['chunk_type']
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            # åˆ›å»ºæ–‡æ¡£æ ‡é¢˜è¡Œï¼ŒåŒ…å«åˆ é™¤æŒ‰é’®
            col1, col2 = st.columns([4, 1])
            with col1:
                doc_expander = st.expander(f"ğŸ“– {doc_name} ({len(chunks)} ä¸ªåˆ‡ç‰‡)", expanded=False)
            with col2:
                if st.button(f"ğŸ—‘ï¸ åˆ é™¤", key=f"delete_{doc_name}", help=f"åˆ é™¤æ–‡æ¡£ {doc_name} çš„æ‰€æœ‰åˆ‡ç‰‡"):
                    st.session_state[f"confirm_delete_{doc_name}"] = True
                    st.rerun()
            
            # å¦‚æœå¤„äºç¡®è®¤åˆ é™¤çŠ¶æ€ï¼Œæ˜¾ç¤ºç¡®è®¤æŒ‰é’®
            if st.session_state.get(f"confirm_delete_{doc_name}", False):
                st.warning(f"âš ï¸ ç¡®è®¤åˆ é™¤æ–‡æ¡£ '{doc_name}' çš„æ‰€æœ‰ {len(chunks)} ä¸ªåˆ‡ç‰‡ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤ï¼")
                col1, col2, col3 = st.columns([1, 1, 2])
                with col1:
                    if st.button("âœ… ç¡®è®¤åˆ é™¤", key=f"confirm_yes_{doc_name}"):
                        try:
                            deleted_count = db_manager.delete_documents_by_source(doc_name)
                            if deleted_count > 0:
                                st.success(f"æˆåŠŸåˆ é™¤æ–‡æ¡£ '{doc_name}' çš„ {deleted_count} ä¸ªåˆ‡ç‰‡")
                                # æ¸…é™¤ç¡®è®¤çŠ¶æ€
                                del st.session_state[f"confirm_delete_{doc_name}"]
                                st.rerun()
                            else:
                                st.error(f"åˆ é™¤æ–‡æ¡£ '{doc_name}' å¤±è´¥ï¼šæœªæ‰¾åˆ°ç›¸å…³åˆ‡ç‰‡")
                        except Exception as e:
                            st.error(f"åˆ é™¤æ–‡æ¡£æ—¶å‡ºé”™: {e}")
                with col2:
                    if st.button("âŒ å–æ¶ˆ", key=f"confirm_no_{doc_name}"):
                        del st.session_state[f"confirm_delete_{doc_name}"]
                        st.rerun()
                continue
            
            # æ–‡æ¡£æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
            with doc_expander:
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**åˆ‡ç‰‡ç±»å‹ç»Ÿè®¡:**")
                    for chunk_type, count in chunk_types.items():
                        st.write(f"- {chunk_type}: {count}")
                
                with col2:
                    st.write("**é¡µé¢èŒƒå›´:**")
                    pages = set(chunk['page_idx'] for chunk in chunks if chunk['page_idx'] is not None)
                    if pages:
                        if len(pages) == 1:
                            st.write(f"ç¬¬ {list(pages)[0]} é¡µ")
                        else:
                            st.write(f"ç¬¬ {min(pages)} - {max(pages)} é¡µ")
                    else:
                        st.write("é¡µé¢ä¿¡æ¯ä¸å¯ç”¨")
                
                st.divider()
                
                # æ˜¾ç¤ºåˆ‡ç‰‡è¯¦æƒ…
                st.write("**åˆ‡ç‰‡è¯¦æƒ…:**")
                
                # å¯¹åˆ‡ç‰‡è¿›è¡Œå»é‡å¤„ç†ï¼Œåˆå¹¶åŒå…¥åº“çš„åˆ‡ç‰‡
                deduplicated_chunks = deduplicate_chunks_for_display(chunks)
                
                for chunk_info in deduplicated_chunks:
                    chunk_type = chunk_info['chunk_type']
                    page_idx = chunk_info['page_idx']
                    chunk_idx_display = chunk_info['chunk_idx_display']
                    primary_chunk = chunk_info['primary_chunk']
                    secondary_chunk = chunk_info.get('secondary_chunk')
                    
                    # æ¯ä¸ªåˆ‡ç‰‡ä½¿ç”¨ç‹¬ç«‹çš„expander
                    with st.expander(f"åˆ‡ç‰‡ {chunk_idx_display} (é¡µé¢ {page_idx}, ç±»å‹: {chunk_type})", expanded=False):
                        chunk = primary_chunk  # ä½¿ç”¨ä¸»åˆ‡ç‰‡è¿›è¡Œæ˜¾ç¤º
                        if chunk_type == 'image':
                            # ä½¿ç”¨åˆ—å¸ƒå±€ï¼Œå›¾ç‰‡åœ¨å·¦ä¾§ï¼Œæè¿°åœ¨å³ä¾§
                            col1, col2 = st.columns([1, 1])
                            
                            with col1:
                                # æ˜¾ç¤ºå›¾ç‰‡
                                try:
                                    image_displayed = False
                                    
                                    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰original_content (base64æ•°æ®)
                                    if 'original_content' in chunk['metadata']:
                                        try:
                                            import base64
                                            import io
                                            from PIL import Image
                                            
                                            original_content = chunk['metadata']['original_content']
                                            if original_content:
                                                # è§£ç Base64å›¾ç‰‡
                                                image_data = base64.b64decode(original_content)
                                                image = Image.open(io.BytesIO(image_data))
                                                st.image(image, caption=f"å›¾ç‰‡åˆ‡ç‰‡ {chunk_idx_display}", use_container_width=True)
                                                image_displayed = True
                                        except Exception as b64_error:
                                            st.warning(f"è§£ç base64å›¾ç‰‡å¤±è´¥: {b64_error}")
                                    
                                    # å¦‚æœbase64æ˜¾ç¤ºå¤±è´¥ï¼Œå°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½
                                    if not image_displayed and 'image_path' in chunk['metadata']:
                                        image_path = chunk['metadata']['image_path']
                                        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•æ„å»ºç»å¯¹è·¯å¾„
                                        if not os.path.isabs(image_path) and chunk.get('content_path'):
                                            # ä½¿ç”¨content_pathä½œä¸ºç»å¯¹è·¯å¾„
                                            full_image_path = chunk['content_path']
                                        else:
                                            full_image_path = image_path
                                        
                                        if os.path.exists(full_image_path):
                                            st.image(full_image_path, caption=f"å›¾ç‰‡åˆ‡ç‰‡ {chunk_idx_display}", use_container_width=True)
                                            image_displayed = True
                                        else:
                                            st.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {full_image_path}")
                                    
                                    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                                    if not image_displayed:
                                        st.warning("æ— æ³•æ˜¾ç¤ºå›¾ç‰‡ï¼šç¼ºå°‘å›¾ç‰‡æ•°æ®")
                                        
                                except Exception as img_error:
                                    st.error(f"æ˜¾ç¤ºå›¾ç‰‡æ—¶å‡ºé”™: {img_error}")
                            
                            with col2:
                                # æ˜¾ç¤ºå›¾ç‰‡çš„æ–‡æœ¬æè¿°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                                content = chunk.get('content', '')
                                if content and content.strip():
                                    st.text_area(
                                        "å›¾ç‰‡æè¿°",
                                        content,
                                        height=200,
                                        disabled=False
                                    )
                        
                        elif chunk_type == 'table':
                            # ä½¿ç”¨åˆ—å¸ƒå±€ï¼Œè¡¨æ ¼å›¾ç‰‡åœ¨å·¦ä¾§ï¼Œæ–‡æœ¬å†…å®¹åœ¨å³ä¾§
                            col1, col2 = st.columns([1, 1])
                            
                            with col1:
                                # å¦‚æœæœ‰è¡¨æ ¼å›¾ç‰‡ï¼Œæ˜¾ç¤ºå›¾ç‰‡
                                if 'table_image_path' in chunk['metadata'] or 'image_path' in chunk['metadata']:
                                    # ä¼˜å…ˆä½¿ç”¨table_image_pathï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨image_path
                                    image_path = chunk['metadata'].get('table_image_path') or chunk['metadata'].get('image_path')
                                    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•æ„å»ºç»å¯¹è·¯å¾„
                                    if not os.path.isabs(image_path) and chunk.get('content_path'):
                                        # ä½¿ç”¨content_pathä½œä¸ºç»å¯¹è·¯å¾„
                                        full_image_path = chunk['content_path']
                                    else:
                                        full_image_path = image_path
                                    
                                    if os.path.exists(full_image_path):
                                        st.image(full_image_path, caption=f"è¡¨æ ¼å›¾ç‰‡", use_container_width=True)
                                    else:
                                        st.warning(f"è¡¨æ ¼å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {full_image_path}")
                            
                            with col2:
                                # æ˜¾ç¤ºè¡¨æ ¼æ–‡æœ¬å†…å®¹
                                content = chunk.get('content', '')
                                if content and content.strip():
                                    st.text_area(
                                        "è¡¨æ ¼æ–‡æœ¬å†…å®¹",
                                        content,
                                        height=200,
                                        disabled=False
                                    )
                        
                        else:
                            # æ˜¾ç¤ºæ–‡æœ¬å†…å®¹
                            content = chunk['content']
                            if content:
                                # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                                display_content = content[:500] + "..." if len(content) > 500 else content
                                st.text_area(
                                    f"æ–‡æœ¬å†…å®¹",
                                    display_content,
                                    height=100,
                                    disabled=False
                                )
                        
                        # æ˜¾ç¤ºå…ƒæ•°æ®
                        with st.expander(f"å…ƒæ•°æ®", expanded=False):
                            metadata_display = {k: v for k, v in chunk['metadata'].items() 
                                              if k not in ['original_content']}  # æ’é™¤base64æ•°æ®
                            st.json(metadata_display)
    
    # æ•°æ®åº“æ“ä½œ
    st.subheader("ğŸ› ï¸ æ•°æ®åº“æ“ä½œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”„ åˆ·æ–°çŸ¥è¯†åº“", type="secondary"):
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“", type="secondary"):
            if st.session_state.get('confirm_clear_kb', False):
                try:
                    db_manager.clear_collection()
                    st.success("çŸ¥è¯†åº“å·²æ¸…ç©º")
                    st.session_state.confirm_clear_kb = False
                    st.rerun()
                except Exception as e:
                    st.error(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            else:
                st.session_state.confirm_clear_kb = True
                st.warning("è¯·å†æ¬¡ç‚¹å‡»ç¡®è®¤æ¸…ç©ºçŸ¥è¯†åº“")
    
    if st.session_state.get('confirm_clear_kb', False):
        st.warning("âš ï¸ æ³¨æ„ï¼šæ¸…ç©ºæ“ä½œä¸å¯æ¢å¤ï¼")

def deduplicate_chunks_for_display(chunks: List[Dict]) -> List[Dict]:
    """å¯¹çŸ¥è¯†åº“ç®¡ç†é¡µé¢çš„åˆ‡ç‰‡è¿›è¡Œå»é‡å¤„ç†ï¼Œåˆå¹¶åŒå…¥åº“çš„åˆ‡ç‰‡
    
    Args:
        chunks: åŸå§‹åˆ‡ç‰‡åˆ—è¡¨
        
    Returns:
        å»é‡åçš„åˆ‡ç‰‡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
        - chunk_type: åˆ‡ç‰‡ç±»å‹
        - page_idx: é¡µé¢ç´¢å¼•
        - chunk_idx_display: æ˜¾ç¤ºç”¨çš„åˆ‡ç‰‡ç´¢å¼•ï¼ˆå¦‚"5-6"è¡¨ç¤ºåŒå…¥åº“ï¼‰
        - primary_chunk: ä¸»åˆ‡ç‰‡ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        - secondary_chunk: æ¬¡åˆ‡ç‰‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    """
    if not chunks:
        return []
    
    deduplicated = []
    processed_chunks = set()
    
    for chunk in chunks:
        chunk_id = chunk.get('chunk_id', '') or chunk.get('id', '')
        if chunk_id in processed_chunks:
            continue
            
        chunk_type = chunk['chunk_type']
        metadata = chunk.get('metadata', {})
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒå…¥åº“çš„åˆ‡ç‰‡ï¼ˆä»…å¯¹imageå’Œtableç±»å‹è¿›è¡ŒåŒå…¥åº“åˆå¹¶ï¼‰
        if chunk_type in ['image', 'table']:
            embedding_type = metadata.get('embedding_type', '')
            
            if embedding_type == 'visual':
                # è¿™æ˜¯è§†è§‰embeddingåˆ‡ç‰‡ï¼Œå¯»æ‰¾å¯¹åº”çš„æ–‡æœ¬embeddingåˆ‡ç‰‡
                text_chunk = None
                
                # ä½¿ç”¨æ›´ç¨³å®šçš„æ ‡è¯†ç¬¦æ¥æŸ¥æ‰¾é…å¯¹åˆ‡ç‰‡
                source_file = metadata.get('source_file', '')
                page_idx = chunk['page_idx']
                chunk_strategy = metadata.get('chunk_strategy', '')
                
                # æ„å»ºåŸºç¡€æ ‡è¯†ç¬¦
                if chunk_type == 'image':
                    image_path = metadata.get('image_path', '')
                    base_identifier = f"{chunk_type}_{source_file}_{page_idx}_{image_path}"
                else:  # table
                    table_image_path = metadata.get('table_image_path', '')
                    base_identifier = f"{chunk_type}_{source_file}_{page_idx}_{table_image_path}"
                
                # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æœ¬embeddingåˆ‡ç‰‡
                for other_chunk in chunks:
                    other_metadata = other_chunk.get('metadata', {})
                    if (other_chunk['chunk_type'] == chunk_type and
                        other_chunk['page_idx'] == page_idx and
                        other_metadata.get('embedding_type') == 'text' and
                        other_metadata.get('source_file') == source_file):
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªå›¾åƒ/è¡¨æ ¼çš„æ–‡æœ¬ç‰ˆæœ¬
                        if chunk_type == 'image':
                            other_image_path = other_metadata.get('image_path', '')
                            if other_image_path == image_path:
                                text_chunk = other_chunk
                                break
                        else:  # table
                            other_table_image_path = other_metadata.get('table_image_path', '')
                            if other_table_image_path == table_image_path:
                                text_chunk = other_chunk
                                break
                
                if text_chunk:
                    # æ‰¾åˆ°äº†é…å¯¹çš„åˆ‡ç‰‡ï¼Œåˆå¹¶æ˜¾ç¤º
                    chunk_info = {
                        'chunk_type': chunk_type,
                        'page_idx': chunk['page_idx'],
                        'chunk_idx_display': f"{chunk['chunk_idx']}-{text_chunk['chunk_idx']}",
                        'primary_chunk': chunk,  # ä½¿ç”¨è§†è§‰åˆ‡ç‰‡ä½œä¸ºä¸»åˆ‡ç‰‡
                        'secondary_chunk': text_chunk
                    }
                    processed_chunks.add(chunk_id)
                    processed_chunks.add(text_chunk.get('chunk_id', '') or text_chunk.get('id', ''))
                else:
                    # æ²¡æœ‰æ‰¾åˆ°é…å¯¹åˆ‡ç‰‡ï¼Œå•ç‹¬æ˜¾ç¤º
                    chunk_info = {
                        'chunk_type': chunk_type,
                        'page_idx': chunk['page_idx'],
                        'chunk_idx_display': str(chunk['chunk_idx']),
                        'primary_chunk': chunk
                    }
                    processed_chunks.add(chunk_id)
                    
            elif embedding_type == 'text':
                # è¿™æ˜¯æ–‡æœ¬embeddingåˆ‡ç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»è¢«å¤„ç†è¿‡
                if chunk_id not in processed_chunks:
                    # å•ç‹¬çš„æ–‡æœ¬åˆ‡ç‰‡ï¼ˆæ²¡æœ‰å¯¹åº”çš„è§†è§‰åˆ‡ç‰‡ï¼‰
                    chunk_info = {
                        'chunk_type': chunk_type,
                        'page_idx': chunk['page_idx'],
                        'chunk_idx_display': str(chunk['chunk_idx']),
                        'primary_chunk': chunk
                    }
                    processed_chunks.add(chunk_id)
                else:
                    continue
            else:
                # æ²¡æœ‰embedding_typeæ ‡è®°çš„åˆ‡ç‰‡ï¼Œå•ç‹¬æ˜¾ç¤º
                chunk_info = {
                    'chunk_type': chunk_type,
                    'page_idx': chunk['page_idx'],
                    'chunk_idx_display': str(chunk['chunk_idx']),
                    'primary_chunk': chunk
                }
                processed_chunks.add(chunk_id)
        else:
            # éimage/tableç±»å‹çš„åˆ‡ç‰‡ï¼ˆå¦‚textç±»å‹ï¼‰ï¼Œç›´æ¥æ˜¾ç¤ºï¼Œä¸è¿›è¡ŒåŒå…¥åº“åˆå¹¶
            chunk_info = {
                'chunk_type': chunk_type,
                'page_idx': chunk['page_idx'],
                'chunk_idx_display': str(chunk['chunk_idx']),
                'primary_chunk': chunk
            }
            processed_chunks.add(chunk_id)
        
        deduplicated.append(chunk_info)
    
    return deduplicated

def deduplicate_search_results(results: List[Dict]) -> List[Dict]:
    """å¯¹æœç´¢ç»“æœè¿›è¡Œå»é‡ï¼Œé¿å…åŒä¸€å›¾åƒæˆ–è¡¨æ ¼çš„é‡å¤ä½¿ç”¨
    ç‰¹åˆ«å¤„ç†åŒå…¥åº“çš„imageå’Œtableç±»åˆ‡ç‰‡ï¼Œåˆå¹¶ç›¸ä¼¼åº¦ä¿¡æ¯
    
    Args:
        results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
        
    Returns:
        å»é‡åçš„æœç´¢ç»“æœåˆ—è¡¨ï¼ŒåŒå…¥åº“çš„åˆ‡ç‰‡ä¼šåŒ…å«ä¸¤ç§ç›¸ä¼¼åº¦ä¿¡æ¯
    """
    if not results:
        return []
    
    # ç”¨äºå­˜å‚¨å·²å¤„ç†çš„åˆ‡ç‰‡
    processed_chunks = {}
    deduplicated_results = []
    
    for result in results:
        metadata = result.get('metadata', {})
        chunk_type = metadata.get('chunk_type', 'unknown')
        
        # å¯¹äºimageå’Œtableç±»å‹ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯åŒå…¥åº“çš„åˆ‡ç‰‡
        if chunk_type in ['image', 'table']:
            # æ„å»ºåˆ‡ç‰‡çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ŒåŸºäºå†…å®¹è€Œä¸æ˜¯chunk_idx
            source_file = metadata.get('source_file', '')
            page_idx = metadata.get('page_idx', '')
            
            # å¯¹äºåŒå…¥åº“çš„åˆ‡ç‰‡ï¼Œä½¿ç”¨æ›´ç¨³å®šçš„æ ‡è¯†ç¬¦
            # åŸºäºchunk_strategyæ¥è¯†åˆ«åŒæºåˆ‡ç‰‡
            chunk_strategy = metadata.get('chunk_strategy', '')
            
            # åˆ›å»ºåŸºç¡€æ ‡è¯†ç¬¦ï¼Œå»é™¤embeddingç›¸å…³çš„åç¼€
            if chunk_strategy.endswith('_visual'):
                base_strategy = chunk_strategy[:-7]  # ç§»é™¤'_visual'
            elif chunk_strategy.endswith('_text') or chunk_strategy.endswith('_caption'):
                base_strategy = chunk_strategy.rsplit('_', 1)[0]  # ç§»é™¤æœ€åä¸€ä¸ªä¸‹åˆ’çº¿åçš„éƒ¨åˆ†
            else:
                base_strategy = chunk_strategy
            
            # å¯¹äºimageç±»å‹ï¼Œè¿˜éœ€è¦è€ƒè™‘image_path
            if chunk_type == 'image':
                image_path = metadata.get('image_path', '')
                base_key = f"{chunk_type}_{source_file}_{page_idx}_{image_path}_{base_strategy}"
            else:  # tableç±»å‹
                table_image_path = metadata.get('table_image_path', '')
                base_key = f"{chunk_type}_{source_file}_{page_idx}_{table_image_path}_{base_strategy}"
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªåˆ‡ç‰‡
            if base_key in processed_chunks:
                # å·²ç»å­˜åœ¨ï¼Œåˆå¹¶ç›¸ä¼¼åº¦ä¿¡æ¯
                existing_result = processed_chunks[base_key]
                embedding_type = metadata.get('embedding_type', 'unknown')
                
                # æ·»åŠ ç¬¬äºŒç§ç›¸ä¼¼åº¦ä¿¡æ¯
                if embedding_type == 'visual':
                    existing_result['visual_similarity'] = 1 - result['distance']
                    existing_result['visual_distance'] = result['distance']
                elif embedding_type == 'text':
                    existing_result['text_similarity'] = 1 - result['distance']
                    existing_result['text_distance'] = result['distance']
                
                # æ ‡è®°ä¸ºåŒå…¥åº“åˆ‡ç‰‡
                existing_result['is_dual_indexed'] = True
            else:
                # ç¬¬ä¸€æ¬¡é‡åˆ°è¿™ä¸ªåˆ‡ç‰‡
                embedding_type = metadata.get('embedding_type', 'unknown')
                
                # å¤åˆ¶ç»“æœå¹¶æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯
                new_result = result.copy()
                new_result['is_dual_indexed'] = False
                
                if embedding_type == 'visual':
                    new_result['visual_similarity'] = 1 - result['distance']
                    new_result['visual_distance'] = result['distance']
                elif embedding_type == 'text':
                    new_result['text_similarity'] = 1 - result['distance']
                    new_result['text_distance'] = result['distance']
                
                # å­˜å‚¨åˆ°å·²å¤„ç†åˆ—è¡¨
                processed_chunks[base_key] = new_result
                deduplicated_results.append(new_result)
        else:
            # éimage/tableç±»å‹ï¼Œä½¿ç”¨åŸæœ‰çš„å»é‡é€»è¾‘
            source_key = None
            
            if metadata.get('source_type') == 'image':
                # å›¾åƒå»é‡ï¼šåŸºäºåŸå§‹æ–‡ä»¶åå’Œé¡µç 
                filename = metadata.get('original_filename') or metadata.get('pdf_filename', '')
                page_num = metadata.get('page_number', '')
                source_key = f"image_{filename}_{page_num}"
            elif metadata.get('source_type') == 'table':
                # è¡¨æ ¼å»é‡ï¼šåŸºäºåŸå§‹æ–‡ä»¶åã€é¡µç å’Œè¡¨æ ¼ç´¢å¼•
                filename = metadata.get('original_filename') or metadata.get('pdf_filename', '')
                page_num = metadata.get('page_number', '')
                table_idx = metadata.get('table_index', '')
                source_key = f"table_{filename}_{page_num}_{table_idx}"
            else:
                # å…¶ä»–ç±»å‹ï¼šåŸºäºæ–‡ä»¶åå’Œå†…å®¹å“ˆå¸Œ
                filename = metadata.get('original_filename') or metadata.get('pdf_filename', '')
                content_hash = hash(result.get('document', '')[:100])  # ä½¿ç”¨å†…å®¹å‰100å­—ç¬¦çš„å“ˆå¸Œ
                source_key = f"other_{filename}_{content_hash}"
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è§è¿‡è¿™ä¸ªæº
            if source_key and source_key not in processed_chunks:
                processed_chunks[source_key] = result
                deduplicated_results.append(result)
    
    return deduplicated_results

def search_similar_images(query: str, db_manager: ChromaDBManager, top_k: int = 3, similarity_threshold: Optional[float] = None, query_type: str = 'text', image_path: Optional[str] = None) -> Optional[List[Dict]]:
    """Search for similar images based on text or image query.
    
    Args:
        query: Search query text or image description.
        db_manager: ChromaDBManager instance.
        top_k: Number of top results to return.
        similarity_threshold: Similarity threshold (0-1), results below this threshold will be filtered out.
        query_type: Type of query ('text' or 'image').
        image_path: Path to image file when query_type is 'image'.
        
    Returns:
        List of search results with metadata, or None if failed.
    """
    try:
        # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©åˆé€‚çš„æœç´¢æ–¹æ³•
        if query_type == 'image' and image_path:
            # ä½¿ç”¨å›¾åƒè¿›è¡Œæœç´¢
            results = db_manager.search_documents(
                query, 
                n_results=top_k * 2,  # è·å–æ›´å¤šç»“æœç”¨äºå»é‡
                similarity_threshold=similarity_threshold,
                embedding_type='visual',
                image_path=image_path
            )
        else:
            # ä½¿ç”¨æ–‡æœ¬è¿›è¡Œæœç´¢
            results = db_manager.search_documents(
                query, 
                n_results=top_k * 2,  # è·å–æ›´å¤šç»“æœç”¨äºå»é‡
                similarity_threshold=similarity_threshold,
                embedding_type='text'
            )
        
        if not results or not results['documents']:
            return None
            
        # Format results
        formatted_results = []
        for i in range(len(results['documents'])):
            result_data = {
                'document': results['documents'][i],
                'distance': results['distances'][i],
                'metadata': results['metadatas'][i],
                'id': results['ids'][i]
            }
            formatted_results.append(result_data)
        
        # å¯¹ç»“æœè¿›è¡Œå»é‡
        deduplicated_results = deduplicate_search_results(formatted_results)
        
        # è¿”å›top_kä¸ªå»é‡åçš„ç»“æœ
        return deduplicated_results[:top_k]
        
    except Exception as e:
        st.error(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™ï¼š{e}")
        return None

def display_chunk_content(result: Dict, index: int):
    """æ ¹æ®chunkç±»å‹å±•ç¤ºå¯¹åº”çš„å†…å®¹ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼ç­‰ï¼‰
    
    Args:
        result: å•ä¸ªæœç´¢ç»“æœå­—å…¸
        index: ç»“æœç´¢å¼•
    """
    metadata = result['metadata']
    source_type = metadata.get('chunk_type', 'unknown')
    similarity = 1 - result['distance']
    filename = metadata.get('source_file', 'æœªçŸ¥')
    
    # æ ¹æ®ç±»å‹è®¾ç½®å›¾æ ‡
    type_icons = {
        'image': 'ğŸ–¼ï¸',
        'table': 'ğŸ“Š', 
        'text': 'ğŸ“'
    }
    icon = type_icons.get(source_type, 'â“')
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒå…¥åº“åˆ‡ç‰‡å¹¶å‡†å¤‡ç›¸ä¼¼åº¦ä¿¡æ¯
    similarity_info = ""
    if result.get('is_dual_indexed', False):
        # åŒå…¥åº“åˆ‡ç‰‡ï¼Œæ˜¾ç¤ºä¸¤ç§ç›¸ä¼¼åº¦
        visual_sim = result.get('visual_similarity')
        text_sim = result.get('text_similarity')
        
        if visual_sim is not None and text_sim is not None:
            similarity_info = f"å›¾åƒ-æ–‡æœ¬: {visual_sim:.3f}, æ–‡æœ¬-æ–‡æœ¬: {text_sim:.3f}"
        elif visual_sim is not None:
            similarity_info = f"å›¾åƒ-æ–‡æœ¬: {visual_sim:.3f}"
        elif text_sim is not None:
            similarity_info = f"æ–‡æœ¬-æ–‡æœ¬: {text_sim:.3f}"
        else:
            similarity_info = f"{similarity:.3f}"
    else:
        # å•ä¸€å…¥åº“åˆ‡ç‰‡ï¼Œæ˜¾ç¤ºå•ä¸€ç›¸ä¼¼åº¦
        embedding_type = metadata.get('embedding_type', 'unknown')
        if embedding_type == 'visual':
            similarity_info = f"å›¾åƒ-æ–‡æœ¬: {similarity:.3f}"
        elif embedding_type == 'text':
            similarity_info = f"æ–‡æœ¬-æ–‡æœ¬: {similarity:.3f}"
        else:
            similarity_info = f"{similarity:.3f}"
    
    # åˆ›å»ºå¯æŠ˜å çš„å‚è€ƒèµ„æ–™å±•ç¤º
    with st.expander(f"{icon} å‚è€ƒèµ„æ–™ {index + 1} - {filename} (ç›¸ä¼¼åº¦: {similarity_info})", expanded=False):
        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        col1, col2 = st.columns([2, 1])
        with col1:
            st.markdown(f"**æ¥æº:** {filename}")
            if metadata.get('page_number'):
                st.markdown(f"**é¡µç :** {metadata['page_number']}")
        with col2:
            st.markdown(f"**ç±»å‹:** {icon} {source_type}")
            if result.get('is_dual_indexed', False):
                st.markdown(f"**åŒå…¥åº“åˆ‡ç‰‡**")
                if result.get('visual_similarity') is not None:
                    st.markdown(f"**å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦:** {result['visual_similarity']:.3f}")
                if result.get('text_similarity') is not None:
                    st.markdown(f"**æ–‡æœ¬-æ–‡æœ¬ç›¸ä¼¼åº¦:** {result['text_similarity']:.3f}")
            else:
                embedding_type = metadata.get('embedding_type', 'unknown')
                if embedding_type == 'visual':
                    st.markdown(f"**å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦:** {similarity:.3f}")
                elif embedding_type == 'text':
                    st.markdown(f"**æ–‡æœ¬-æ–‡æœ¬ç›¸ä¼¼åº¦:** {similarity:.3f}")
                else:
                    st.markdown(f"**ç›¸ä¼¼åº¦:** {similarity:.3f}")
        
        # æ ¹æ®chunkç±»å‹æ˜¾ç¤ºä¸åŒå†…å®¹
        if source_type == 'image':
            # ä½¿ç”¨ä¸‰åˆ—å¸ƒå±€æ˜¾ç¤ºå›¾åƒã€æè¿°å’Œå…¶ä»–ä¿¡æ¯
            col_img, col_desc, col_info = st.columns([1, 2, 1])
            
            with col_img:
                # æ˜¾ç¤ºå›¾åƒ
                if metadata.get('has_original_content', False):
                    try:
                        original_content = metadata.get('original_content')
                        if original_content:
                            import base64
                            import io
                            image_data = base64.b64decode(original_content)
                            image = Image.open(io.BytesIO(image_data))
                            st.image(image, caption="å›¾åƒå†…å®¹", width=200)
                    except Exception as e:
                        st.error(f"æ˜¾ç¤ºå›¾åƒå¤±è´¥: {e}")
            
            with col_desc:
                # æ˜¾ç¤ºå›¾åƒæè¿°
                if result['document']:
                    st.markdown("**å›¾åƒæè¿°:**")
                    st.write(result['document'])
            
            with col_info:
                # æ˜¾ç¤ºé¢å¤–ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if metadata.get('chunk_strategy'):
                    st.markdown(f"**åˆ‡ç‰‡ç­–ç•¥:** {metadata['chunk_strategy']}")
        
        elif source_type == 'table':
            # ä½¿ç”¨ä¸‰åˆ—å¸ƒå±€æ˜¾ç¤ºè¡¨æ ¼å›¾åƒã€å†…å®¹å’Œå…¶ä»–ä¿¡æ¯
            col_img, col_desc, col_info = st.columns([1, 2, 1])
            
            with col_img:
                # æ˜¾ç¤ºè¡¨æ ¼å›¾åƒï¼ˆå¦‚æœæœ‰ï¼‰
                if metadata.get('has_original_content', False):
                    try:
                        original_content = metadata.get('original_content')
                        if original_content:
                            import base64
                            import io
                            image_data = base64.b64decode(original_content)
                            image = Image.open(io.BytesIO(image_data))
                            st.image(image, caption="è¡¨æ ¼å›¾åƒ", width=200)
                    except Exception as e:
                        st.error(f"æ˜¾ç¤ºè¡¨æ ¼å›¾åƒå¤±è´¥: {e}")
            
            with col_desc:
                # æ˜¾ç¤ºè¡¨æ ¼HTMLå†…å®¹
                if result['document']:
                    st.markdown("**è¡¨æ ¼å†…å®¹:**")
                    # å°è¯•æ¸²æŸ“HTMLè¡¨æ ¼
                    if '<table' in result['document'].lower():
                        st.markdown(result['document'], unsafe_allow_html=True)
                    else:
                        st.write(result['document'])
            
            with col_info:
                # æ˜¾ç¤ºè¡¨æ ¼ç´¢å¼•å’Œå…¶ä»–ä¿¡æ¯
                if metadata.get('table_index') is not None:
                    st.markdown(f"**è¡¨æ ¼ç´¢å¼•:** {metadata['table_index']}")
                if metadata.get('chunk_strategy'):
                    st.markdown(f"**åˆ‡ç‰‡ç­–ç•¥:** {metadata['chunk_strategy']}")
        
        elif source_type == 'text':
            # æ˜¾ç¤ºæ–‡æœ¬å†…å®¹
            if result['document']:
                st.markdown("**æ–‡æœ¬å†…å®¹:**")
                st.write(result['document'])
        
        else:
            # æ˜¾ç¤ºé€šç”¨å†…å®¹
            if result['document']:
                st.markdown("**å†…å®¹:**")
                st.write(result['document'])
            
            # å¦‚æœæœ‰å›¾åƒå†…å®¹ï¼Œä¹Ÿæ˜¾ç¤º
            if metadata.get('has_original_content', False):
                try:
                    original_content = metadata.get('original_content')
                    if original_content:
                        import base64
                        import io
                        image_data = base64.b64decode(original_content)
                        image = Image.open(io.BytesIO(image_data))
                        st.image(image, caption="ç›¸å…³å›¾åƒ", width=400)
                except Exception as e:
                    st.error(f"æ˜¾ç¤ºå›¾åƒå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºè¯¦ç»†å…ƒæ•°æ®ï¼ˆå¯æŠ˜å ï¼‰
        with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†å…ƒæ•°æ®", expanded=False):
            for key, value in metadata.items():
                if key not in ['original_content']:  # ä¸æ˜¾ç¤ºBase64å†…å®¹
                    st.write(f"**{key}:** {value}")

def display_search_results(results: List[Dict]):
    """Display search results with images and metadata in a grid layout.
    
    Args:
        results: List of search result dictionaries.
    """
    if not results:
        st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
        return
        
    st.write(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœï¼š")
    
    # Display results in a grid layout (3 columns per row)
    cols_per_row = 3
    for i in range(0, len(results), cols_per_row):
        cols = st.columns(cols_per_row)
        
        for j in range(cols_per_row):
            if i + j < len(results):
                result = results[i + j]
                metadata = result['metadata']
                
                with cols[j]:
                    # Display image from ChromaDB
                    image_displayed = False
                    
                    # ä»ChromaDBè·å–åŸå§‹å†…å®¹
                    if metadata.get('has_original_content', False):
                        try:
                            # ä»å½“å‰ç»“æœçš„å…ƒæ•°æ®ä¸­è·å–åŸå§‹å†…å®¹
                            original_content = metadata.get('original_content')
                            if original_content:
                                import base64
                                import io
                                # è§£ç Base64å›¾ç‰‡
                                image_data = base64.b64decode(original_content)
                                image = Image.open(io.BytesIO(image_data))
                                st.image(
                                    image, 
                                    caption=f"ç›¸ä¼¼åº¦: {1-result['distance']:.3f}",
                                    width=300
                                )
                                image_displayed = True
                        except Exception as e:
                            st.error(f"ä»æ•°æ®åº“æ˜¾ç¤ºå›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœä»æ•°æ®åº“è·å–å¤±è´¥ï¼Œæ˜¾ç¤ºå ä½ç¬¦
                    if not image_displayed:
                        st.info("å›¾ç‰‡ä¸å¯ç”¨")
                    
                    # Display basic info in a compact format
                    with st.expander(f"è¯¦ç»†ä¿¡æ¯ #{i+j+1}", expanded=False):
                        st.write(f"**æ¥æº:** {metadata.get('source_file', 'æœªçŸ¥')}")
                        st.write(f"**å†…å®¹:** {result['document']}")
                        st.write(f"**ç›¸ä¼¼åº¦åˆ†æ•°:** {1-result['distance']:.4f}")
                        st.write(f"**è·ç¦»åˆ†æ•°:** {result['distance']:.4f}")
                        
                        if metadata:
                            st.write("**å…ƒæ•°æ®:**")
                            for key, value in metadata.items():
                                if key not in ['original_content']:  # Don't show Base64 content
                                    st.write(f"- {key}: {value}")

def display_duplicate_confirmation(duplicate_results: Dict[str, Any]) -> bool:
    """æ˜¾ç¤ºé‡å¤æ£€æµ‹ç»“æœå’Œè¦†ç›–ç¡®è®¤ç•Œé¢
    
    Args:
        duplicate_results: é‡å¤æ£€æµ‹ç»“æœ
        
    Returns:
        True if user confirms overwrite, False otherwise
    """
    duplicates = duplicate_results.get('duplicates', [])
    new_items = duplicate_results.get('new_items', [])
    
    if not duplicates and not new_items:
        st.info("æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å†…å®¹")
        return False
    
    st.warning(f"æ£€æµ‹åˆ° {len(duplicates)} ä¸ªé‡å¤é¡¹å’Œ {len(new_items)} ä¸ªæ–°é¡¹ç›®")
    
    if duplicates:
        st.subheader("ğŸ”„ é‡å¤é¡¹ç›®")
        st.write("ä»¥ä¸‹é¡¹ç›®å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼š")
        
        for i, dup in enumerate(duplicates):
            with st.expander(f"é‡å¤é¡¹ {i+1}: {dup['content'][:50]}...", expanded=False):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**æ–°ä¸Šä¼ å†…å®¹:**")
                    st.write(f"å†…å®¹: {dup['content']}")
                    if dup['metadata']:
                        st.write("å…ƒæ•°æ®:")
                        for key, value in dup['metadata'].items():
                            if key not in ['original_content']:
                                st.write(f"- {key}: {value}")
                
                with col2:
                    st.write("**æ•°æ®åº“ä¸­å·²å­˜åœ¨:**")
                    if 'existing_doc' in dup and dup['existing_doc']:
                        existing = dup['existing_doc']
                        st.write(f"å†…å®¹: {existing['document']}")
                        st.write(f"ID: {existing['id']}")
                        if existing['metadata']:
                            st.write("å…ƒæ•°æ®:")
                            for key, value in existing['metadata'].items():
                                if key not in ['original_content']:
                                    st.write(f"- {key}: {value}")
                    else:
                        st.write("æ— æ³•è·å–å·²å­˜åœ¨æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯")
                        st.write(f"é‡å¤é¡¹ID: {dup.get('existing_id', 'æœªçŸ¥')}")
    
    if new_items:
        st.subheader("âœ¨ æ–°é¡¹ç›®")
        st.write(f"ä»¥ä¸‹ {len(new_items)} ä¸ªé¡¹ç›®å°†è¢«æ·»åŠ åˆ°æ•°æ®åº“ï¼š")
        
        for i, item in enumerate(new_items):
            with st.expander(f"æ–°é¡¹ç›® {i+1}: {item['content'][:50]}...", expanded=False):
                st.write(f"å†…å®¹: {item['content']}")
                if item['metadata']:
                    st.write("å…ƒæ•°æ®:")
                    for key, value in item['metadata'].items():
                        if key not in ['original_content']:
                            st.write(f"- {key}: {value}")
    
    st.markdown("---")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ”„ è¦†ç›–é‡å¤é¡¹å¹¶æ·»åŠ æ–°é¡¹ç›®", type="primary"):
            return True
    
    with col2:
        if st.button("â• ä»…æ·»åŠ æ–°é¡¹ç›®"):
            return "new_only"
    
    with col3:
        if st.button("âŒ å–æ¶ˆä¸Šä¼ "):
            st.session_state.show_duplicate_confirmation = False
            st.session_state.duplicate_check_results = None
            st.session_state.pending_uploads = None
            st.rerun()
    
    return False

def initialize_session_state():
    """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€å˜é‡"""
    if 'config' not in st.session_state:
        st.session_state.config = VisionRAGConfig()
    if 'db_manager' not in st.session_state:
        st.session_state.db_manager = None
    if 'embedding_tool' not in st.session_state:
        st.session_state.embedding_tool = None
    if 'answer_agent' not in st.session_state:
        st.session_state.answer_agent = None
    if 'processed_files' not in st.session_state:
        st.session_state.processed_files = set()
    if 'num_results' not in st.session_state:
        st.session_state.num_results = st.session_state.config.default_num_results
    if 'similarity_threshold' not in st.session_state:
        st.session_state.similarity_threshold = st.session_state.config.default_similarity_threshold
    if 'use_similarity_threshold' not in st.session_state:
        st.session_state.use_similarity_threshold = False
    if 'search_results' not in st.session_state:
        st.session_state.search_results = None
    if 'pending_uploads' not in st.session_state:
        st.session_state.pending_uploads = None
    if 'duplicate_check_results' not in st.session_state:
        st.session_state.duplicate_check_results = None
    if 'show_duplicate_confirmation' not in st.session_state:
        st.session_state.show_duplicate_confirmation = False

def setup_sidebar():
    """è®¾ç½®ä¾§è¾¹æ UIå¹¶è¿”å›è®¾ç½®"""
    with st.sidebar:
        st.header("ğŸ”§ è®¾ç½®")
        
        # Configuration management
        st.subheader("é…ç½®ç®¡ç†")
        config = st.session_state.config
        
        # Database settings
        st.subheader("æ•°æ®åº“è®¾ç½®")
        db_path = st.text_input(
            "æ•°æ®åº“è·¯å¾„", 
            value=config.db_path,
            help="å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„ï¼Œå»ºè®®ä½¿ç”¨æŒä¹…åŒ–ç›®å½•"
        )
        collection_name = st.text_input(
            "é›†åˆåç§°", 
            value=config.collection_name,
            help="æ•°æ®åº“é›†åˆåç§°ï¼Œä¸åŒé¡¹ç›®å»ºè®®ä½¿ç”¨ä¸åŒåç§°"
        )
        
        # Update config
        config.db_path = db_path
        config.collection_name = collection_name
        
        st.markdown("---")
        
        # Search settings
        st.subheader("æœç´¢è®¾ç½®")
        num_results = st.slider(
            "è¿”å›ç»“æœæ•°é‡",
            min_value=1,
            max_value=config.max_num_results,
            value=st.session_state.num_results,
            help="è®¾ç½®æ£€ç´¢è¿”å›çš„ç›¸å…³å›¾ç‰‡æ•°é‡"
        )
        st.session_state.num_results = num_results
        
        # Similarity threshold settings
        use_similarity_threshold = st.checkbox(
            "å¯ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤",
            value=st.session_state.use_similarity_threshold,
            help="å¯ç”¨åï¼Œåªè¿”å›ç›¸ä¼¼åº¦é«˜äºè®¾å®šé˜ˆå€¼çš„ç»“æœ"
        )
        st.session_state.use_similarity_threshold = use_similarity_threshold
        
        if use_similarity_threshold:
            similarity_threshold = st.slider(
                "ç›¸ä¼¼åº¦é˜ˆå€¼",
                min_value=0.0,
                max_value=1.0,
                value=st.session_state.similarity_threshold,
                step=0.05,
                help="è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œåªè¿”å›ç›¸ä¼¼åº¦é«˜äºæ­¤å€¼çš„ç»“æœ"
            )
            st.session_state.similarity_threshold = similarity_threshold
        
        # Auto expand settings
        st.subheader("æŸ¥è¯¢æ‰©å†™è®¾ç½®")
        enable_auto_expand = st.checkbox(
            "å¯ç”¨è‡ªåŠ¨æ‰©å†™",
            value=config.enable_auto_expand,
            help="å½“æŸ¥è¯¢å°‘äºæŒ‡å®šå­—æ•°æ—¶ï¼Œè‡ªåŠ¨æ‰©å†™æŸ¥è¯¢ä»¥æé«˜æ£€ç´¢æ•ˆæœ"
        )
        config.enable_auto_expand = enable_auto_expand
        
        if enable_auto_expand:
            auto_expand_min_length = st.number_input(
                "è‡ªåŠ¨æ‰©å†™æœ€ä½å­—æ•°",
                min_value=10,
                max_value=200,
                value=config.auto_expand_min_length,
                step=10,
                help="å½“æŸ¥è¯¢å­—æ•°å°‘äºæ­¤å€¼æ—¶ï¼Œå°†è§¦å‘è‡ªåŠ¨æ‰©å†™"
            )
            config.auto_expand_min_length = auto_expand_min_length
        
        # Advanced options
        if st.checkbox("æ˜¾ç¤ºé«˜çº§é€‰é¡¹", value=config.show_advanced_options):
            config.show_advanced_options = True
            
            st.subheader("æ–‡æ¡£è§£æé…ç½®")
            config.output_dir = st.text_input(
                "è¾“å‡ºç›®å½•", 
                value=config.output_dir,
                help="æ–‡æ¡£è§£æç»“æœçš„è¾“å‡ºç›®å½•"
            )
            config.cls_dir = st.text_input(
                "åˆ†ç±»ç›®å½•", 
                value=config.cls_dir,
                help="çŸ¥è¯†åº“åˆ†ç±»ç›®å½•åç§°"
            )
            config.lang = st.selectbox(
                "è§£æè¯­è¨€",
                options=["ch", "en"],
                index=0 if config.lang == "ch" else 1,
                help="æ–‡æ¡£è§£æè¯­è¨€"
            )
            
            col1, col2, col3 = st.columns(3)
            with col1:
                config.enable_formula = st.checkbox(
                    "å¯ç”¨å…¬å¼è§£æ", 
                    value=config.enable_formula,
                    help="é€‚ç”¨äºå­¦æœ¯æ–‡æ¡£"
                )
            with col2:
                config.enable_table = st.checkbox(
                    "å¯ç”¨è¡¨æ ¼è§£æ", 
                    value=config.enable_table,
                    help="é€‚ç”¨äºåŒ…å«è¡¨æ ¼çš„æ–‡æ¡£"
                )
            with col3:
                config.auto_caption = st.checkbox(
                    "è‡ªåŠ¨å›¾ç‰‡æè¿°", 
                    value=config.auto_caption,
                    help="æé«˜æ£€ç´¢å‡†ç¡®æ€§"
                )
            
            st.subheader("åˆ‡ç‰‡é…ç½®")
            config.max_chunk_size = st.number_input(
                "æœ€å¤§åˆ‡ç‰‡å¤§å°",
                min_value=100,
                max_value=5000,
                value=config.max_chunk_size,
                step=100,
                help="å•ä¸ªæ–‡æœ¬åˆ‡ç‰‡çš„æœ€å¤§å­—ç¬¦æ•°"
            )
            config.min_chunk_size = st.number_input(
                "æœ€å°åˆ‡ç‰‡å¤§å°",
                min_value=50,
                max_value=1000,
                value=config.min_chunk_size,
                step=50,
                help="å•ä¸ªæ–‡æœ¬åˆ‡ç‰‡çš„æœ€å°å­—ç¬¦æ•°"
            )
            config.overlap_size = st.number_input(
                "é‡å å¤§å°",
                min_value=0,
                max_value=500,
                value=config.overlap_size,
                step=50,
                help="ç›¸é‚»åˆ‡ç‰‡çš„é‡å å­—ç¬¦æ•°"
            )
            
            col1, col2 = st.columns(2)
            with col1:
                config.preserve_sentences = st.checkbox(
                    "ä¿æŒå¥å­å®Œæ•´æ€§", 
                    value=config.preserve_sentences
                )
            with col2:
                config.preserve_paragraphs = st.checkbox(
                    "ä¿æŒæ®µè½å®Œæ•´æ€§", 
                    value=config.preserve_paragraphs
                )
        else:
            config.show_advanced_options = False
        
        st.markdown("---")
        
        # Database info
        if st.session_state.db_manager:
            st.subheader("æ•°æ®åº“ä¿¡æ¯")
            info = st.session_state.db_manager.get_collection_info()
            st.write(f"é›†åˆåç§°: {info.get('name', 'N/A')}")
            st.write(f"åˆ‡ç‰‡æ•°é‡: {info.get('count', 0)}")
            
            if st.button("æ¸…ç©ºæ•°æ®åº“", type="secondary"):
                if st.session_state.db_manager.clear_collection():
                    st.success("æ•°æ®åº“å·²æ¸…ç©º")
                    st.rerun()
                else:
                    st.error("æ¸…ç©ºæ•°æ®åº“å¤±è´¥")
    
    return db_path, collection_name

def initialize_tools(db_path: str, collection_name: str):
    """åˆå§‹åŒ–åµŒå…¥å·¥å…·ã€æ•°æ®åº“ç®¡ç†å™¨å’Œé—®ç­”åŠ©æ‰‹"""
    if not TongyiEmbedding or not ChromaDBManager or not AnswerAgent:
        st.error("æ— æ³•å¯¼å…¥å¿…è¦çš„å·¥å…·ç±»ï¼Œè¯·æ£€æŸ¥ä¾èµ–")
        return False
        
    try:
        # Initialize embedding tool
        if st.session_state.embedding_tool is None:
            st.session_state.embedding_tool = TongyiEmbedding()
            st.sidebar.success("é€šä¹‰åµŒå…¥å·¥å…·åˆå§‹åŒ–æˆåŠŸï¼")
        
        # Initialize database manager
        if st.session_state.db_manager is None:
            st.session_state.db_manager = ChromaDBManager(
                embedding_tool=st.session_state.embedding_tool,
                db_path=db_path,
                collection_name=collection_name
            )
            st.sidebar.success("ChromaDBç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼")
        
        # Initialize answer agent
        if st.session_state.answer_agent is None:
            st.session_state.answer_agent = AnswerAgent()
            st.sidebar.success("é—®ç­”åŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸï¼")
        
        return True
        
    except Exception as e:
        st.sidebar.error(f"åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        return False

@dataclass
class VisionRAGConfig:
    """Vision RAGç³»ç»Ÿé…ç½®ç±»"""
    
    # æ•°æ®è§£æé…ç½®
    output_dir: str = "data/output"  # è§£æè¾“å‡ºç›®å½•
    cls_dir: str = "default"  # çŸ¥è¯†åº“åˆ†ç±»ç›®å½•
    lang: str = "ch"  # è§£æè¯­è¨€ ("ch" ä¸­æ–‡, "en" è‹±æ–‡)
    enable_formula: bool = True  # å¯ç”¨å…¬å¼è§£æ
    enable_table: bool = True  # å¯ç”¨è¡¨æ ¼è§£æ
    auto_caption: bool = True  # è‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡æè¿°
    
    # åˆ‡ç‰‡é…ç½®
    max_chunk_size: int = 1000  # æœ€å¤§åˆ‡ç‰‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    min_chunk_size: int = 100   # æœ€å°åˆ‡ç‰‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    overlap_size: int = 100     # é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    preserve_sentences: bool = True  # ä¿æŒå¥å­å®Œæ•´æ€§
    preserve_paragraphs: bool = True  # ä¿æŒæ®µè½å®Œæ•´æ€§
    
    # å‘é‡æ•°æ®åº“é…ç½®
    db_path: str = "./chromadb_data"  # æ•°æ®åº“å­˜å‚¨è·¯å¾„
    collection_name: str = "vision_rag_documents"  # é›†åˆåç§°
    
    # æ£€ç´¢é…ç½®
    default_num_results: int = 5  # é»˜è®¤æ£€ç´¢ç»“æœæ•°é‡
    max_num_results: int = 20  # æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡
    default_similarity_threshold: float = 0.7  # é»˜è®¤ç›¸ä¼¼åº¦é˜ˆå€¼
    enable_query_expansion: bool = True  # å¯ç”¨æŸ¥è¯¢æ‰©å±•
    
    # è‡ªåŠ¨æ‰©å†™é…ç½®
    enable_auto_expand: bool = False  # å¯ç”¨è‡ªåŠ¨æ‰©å†™
    auto_expand_min_length: int = 50  # è‡ªåŠ¨æ‰©å†™æœ€ä½å­—æ•°
    
    # UIé…ç½®
    show_advanced_options: bool = False  # æ˜¾ç¤ºé«˜çº§é€‰é¡¹
    show_debug_info: bool = False  # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
    
    @classmethod
    def get_config_descriptions(cls) -> Dict[str, str]:
        """è·å–é…ç½®é¡¹è¯´æ˜"""
        return {
            "output_dir": "æ–‡æ¡£è§£æç»“æœçš„è¾“å‡ºç›®å½•ï¼Œå»ºè®®ä½¿ç”¨ç›¸å¯¹è·¯å¾„",
            "cls_dir": "çŸ¥è¯†åº“åˆ†ç±»ç›®å½•åç§°ï¼Œç”¨äºç»„ç»‡ä¸åŒç±»å‹çš„æ–‡æ¡£",
            "lang": "æ–‡æ¡£è§£æè¯­è¨€ï¼Œæ”¯æŒä¸­æ–‡(ch)å’Œè‹±æ–‡(en)",
            "enable_formula": "æ˜¯å¦å¯ç”¨æ•°å­¦å…¬å¼è§£æï¼Œé€‚ç”¨äºå­¦æœ¯æ–‡æ¡£",
            "enable_table": "æ˜¯å¦å¯ç”¨è¡¨æ ¼è§£æï¼Œé€‚ç”¨äºåŒ…å«è¡¨æ ¼çš„æ–‡æ¡£",
            "auto_caption": "æ˜¯å¦è‡ªåŠ¨ä¸ºå›¾ç‰‡ç”Ÿæˆæè¿°ï¼Œæé«˜æ£€ç´¢å‡†ç¡®æ€§",
            "max_chunk_size": "å•ä¸ªæ–‡æœ¬åˆ‡ç‰‡çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œå½±å“æ£€ç´¢ç²’åº¦",
            "min_chunk_size": "å•ä¸ªæ–‡æœ¬åˆ‡ç‰‡çš„æœ€å°å­—ç¬¦æ•°ï¼Œé¿å…è¿‡å°çš„ç‰‡æ®µ",
            "overlap_size": "ç›¸é‚»åˆ‡ç‰‡çš„é‡å å­—ç¬¦æ•°ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§",
            "preserve_sentences": "åˆ‡ç‰‡æ—¶ä¿æŒå¥å­å®Œæ•´æ€§ï¼Œæé«˜è¯­ä¹‰è¿è´¯æ€§",
            "preserve_paragraphs": "åˆ‡ç‰‡æ—¶ä¿æŒæ®µè½å®Œæ•´æ€§ï¼Œé€‚åˆç»“æ„åŒ–æ–‡æ¡£",
            "db_path": "å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„ï¼Œå»ºè®®ä½¿ç”¨æŒä¹…åŒ–ç›®å½•",
            "collection_name": "æ•°æ®åº“é›†åˆåç§°ï¼Œä¸åŒé¡¹ç›®å»ºè®®ä½¿ç”¨ä¸åŒåç§°",
            "default_num_results": "é»˜è®¤æ£€ç´¢è¿”å›çš„ç»“æœæ•°é‡",
            "max_num_results": "æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡é™åˆ¶",
            "default_similarity_threshold": "é»˜è®¤ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¿‡æ»¤ä½ç›¸å…³æ€§ç»“æœ",
            "enable_query_expansion": "å¯ç”¨æŸ¥è¯¢æ‰©å±•ï¼Œæé«˜æ£€ç´¢å¬å›ç‡",
            "enable_auto_expand": "å¯ç”¨è‡ªåŠ¨æ‰©å†™åŠŸèƒ½ï¼Œå½“æŸ¥è¯¢å°‘äºæŒ‡å®šå­—æ•°æ—¶è‡ªåŠ¨æ‰©å†™",
            "auto_expand_min_length": "è‡ªåŠ¨æ‰©å†™çš„æœ€ä½å­—æ•°è¦æ±‚ï¼Œå°‘äºæ­¤å­—æ•°å°†è§¦å‘æ‰©å†™",
            "show_advanced_options": "åœ¨ç•Œé¢ä¸­æ˜¾ç¤ºé«˜çº§é…ç½®é€‰é¡¹",
            "show_debug_info": "æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯ï¼Œç”¨äºå¼€å‘å’Œè°ƒè¯•"
        }

def main():
    """ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title="Vision RAG with Qwen",
        page_icon="ğŸ”",
        layout="wide"
    )
    
    st.title("ğŸ” Vision RAG with Qwen")
    st.markdown("åŸºäºé€šä¹‰åƒé—®å’ŒChromaDBçš„è§†è§‰æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ")
    
    # Initialize session state
    initialize_session_state()
    
    # Setup sidebar
    db_path, collection_name = setup_sidebar()
    
    # Initialize tools
    if not initialize_tools(db_path, collection_name):
        st.stop()
    
    # Main interface
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ ä¸Šä¼ æ–‡æ¡£", "ğŸ” æœç´¢æŸ¥è¯¢", "ğŸ“š çŸ¥è¯†åº“ç®¡ç†"])
    
    with tab1:
        st.header("ä¸Šä¼ å›¾ç‰‡æˆ–PDFæ–‡æ¡£")
        
        # æ˜¾ç¤ºé‡å¤æ£€æµ‹ç¡®è®¤ç•Œé¢
        # åˆ‡ç‰‡å…¥åº“ç¡®è®¤ç•Œé¢
        if st.session_state.show_duplicate_confirmation and st.session_state.duplicate_check_results:
            st.subheader("ğŸ” åˆ‡ç‰‡å…¥åº“æ£€æµ‹ç»“æœ")
            
            confirmation_result = display_duplicate_confirmation(st.session_state.duplicate_check_results)
            
            if confirmation_result == True:  # è¦†ç›–é‡å¤é¡¹å¹¶æ·»åŠ æ–°é¡¹ç›®
                # åˆ›å»ºè¿›åº¦å®¹å™¨
                progress_container = st.container()
                
                with progress_container:
                    # è¿›åº¦æ¡
                    progress_bar = st.progress(0.0)
                    status_text = st.empty()
                    
                    try:
                        duplicates = st.session_state.duplicate_check_results.get('duplicates', [])
                        new_items = st.session_state.duplicate_check_results.get('new_items', [])
                        all_items = duplicates + new_items
                        
                        status_text.text(f"å‡†å¤‡è¦†ç›–å­˜å‚¨ {len(all_items)} ä¸ªåˆ‡ç‰‡åˆ°æ•°æ®åº“...")
                        progress_bar.progress(0.1)
                        
                        # ç›´æ¥ä½¿ç”¨å·²è§£æçš„åˆ‡ç‰‡æ•°æ®è¿›è¡Œæ‰¹é‡æ·»åŠ ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
                        if all_items:
                            with st.spinner(f"ğŸ’¾ æ­£åœ¨è¦†ç›–å­˜å‚¨ {len(all_items)} ä¸ªåˆ‡ç‰‡åˆ°æ•°æ®åº“"):
                                progress_bar.progress(0.5)
                                result = st.session_state.db_manager.batch_add_with_overwrite(
                                    all_items, force_overwrite=True
                                )
                                progress_bar.progress(0.9)
                            
                            success_count = len(result.get('successful_ids', []))
                            progress_bar.progress(1.0)
                            status_text.text("è¦†ç›–å­˜å‚¨å®Œæˆï¼")
                            
                            st.success(f"ğŸ‰ è¦†ç›–å­˜å‚¨å®Œæˆï¼æˆåŠŸå¤„ç† {success_count} ä¸ªåˆ‡ç‰‡")
                            
                            if success_count > 0:
                                st.balloons()
                        else:
                            progress_bar.progress(1.0)
                            status_text.text("æ²¡æœ‰åˆ‡ç‰‡éœ€è¦å¤„ç†")
                            st.info("â„¹ï¸ æ²¡æœ‰åˆ‡ç‰‡éœ€è¦å¤„ç†")
                            
                    except Exception as e:
                        st.error(f"âŒ è¦†ç›–å­˜å‚¨åˆ‡ç‰‡æ—¶å‡ºé”™ï¼š{e}")
                    finally:
                        # æ¸…ç†è¿›åº¦æ˜¾ç¤º
                        progress_bar.empty()
                        status_text.empty()
                    
                    # æ¸…ç†çŠ¶æ€
                    st.session_state.show_duplicate_confirmation = False
                    st.session_state.duplicate_check_results = None
                    st.session_state.parsed_chunks = None
                    st.rerun()
                    
            elif confirmation_result == "new_only":  # ä»…æ·»åŠ æ–°é¡¹ç›®
                # åˆ›å»ºè¿›åº¦å®¹å™¨
                progress_container = st.container()
                
                with progress_container:
                    # è¿›åº¦æ¡
                    progress_bar = st.progress(0.0)
                    status_text = st.empty()
                    
                    try:
                        duplicates = st.session_state.duplicate_check_results.get('duplicates', [])
                        new_items = st.session_state.duplicate_check_results.get('new_items', [])
                        
                        status_text.text(f"å‡†å¤‡æ·»åŠ  {len(new_items)} ä¸ªæ–°åˆ‡ç‰‡...")
                        progress_bar.progress(0.2)
                        
                        # ä½¿ç”¨æ‰¹é‡æ·»åŠ æ–¹æ³•å¤„ç†æ–°é¡¹ç›®
                        if new_items:
                            with st.spinner(f"ğŸ’¾ æ­£åœ¨æ·»åŠ  {len(new_items)} ä¸ªæ–°åˆ‡ç‰‡åˆ°æ•°æ®åº“"):
                                progress_bar.progress(0.5)
                                result = st.session_state.db_manager.batch_add_with_overwrite(
                                    new_items, force_overwrite=False
                                )
                                progress_bar.progress(0.8)
                            
                            success_count = len(result.get('successful_ids', []))
                            progress_bar.progress(1.0)
                            status_text.text("æ–°é¡¹ç›®æ·»åŠ å®Œæˆï¼")
                            
                            st.success(f"âœ… æˆåŠŸæ·»åŠ  {success_count} ä¸ªæ–°åˆ‡ç‰‡")
                            
                            if success_count > 0:
                                st.balloons()
                        else:
                            progress_bar.progress(1.0)
                            status_text.text("æ²¡æœ‰æ–°é¡¹ç›®éœ€è¦æ·»åŠ ")
                            st.info("â„¹ï¸ æ²¡æœ‰æ–°é¡¹ç›®éœ€è¦æ·»åŠ ")
                            
                    except Exception as e:
                        st.error(f"âŒ æ·»åŠ æ–°é¡¹ç›®æ—¶å‡ºé”™ï¼š{e}")
                    finally:
                        # æ¸…ç†è¿›åº¦æ˜¾ç¤º
                        progress_bar.empty()
                        status_text.empty()
                    
                    # æ¸…ç†çŠ¶æ€
                    st.session_state.show_duplicate_confirmation = False
                    st.session_state.duplicate_check_results = None
                    st.session_state.parsed_chunks = None
                    st.rerun()
        
        else:
            # ç»Ÿä¸€çš„æ–‡æ¡£ä¸Šä¼ ç•Œé¢
            st.subheader("ğŸ“ æ–‡æ¡£ä¸Šä¼ ä¸ç®¡ç†")
            
            # æ–‡ä»¶ä¸Šä¼ ç•Œé¢
            uploaded_files = st.file_uploader(
                "é€‰æ‹©æ–‡ä»¶",
                type=['png', 'jpg', 'jpeg', 'pdf', 'docx', 'doc', 'pptx', 'ppt', 'xlsx', 'xls'],
                accept_multiple_files=True,
                help="æ”¯æŒå›¾ç‰‡(PNGã€JPGã€JPEG)ã€PDFæ–‡æ¡£å’ŒOfficeæ–‡æ¡£(Wordã€PowerPointã€Excel)",
                key="upload_files"
            )
            
            if uploaded_files:
                # è‡ªåŠ¨æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
                with st.spinner("æ­£åœ¨æ£€æŸ¥æ–‡æ¡£çŠ¶æ€..."):
                    parsed_files = []
                    unparsed_files = []
                    
                    for file in uploaded_files:
                        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥æ–‡ä»¶çš„åˆ‡ç‰‡
                        try:
                            # ä½¿ç”¨æ–‡ä»¶åæŸ¥è¯¢æ•°æ®åº“
                            query_result = st.session_state.db_manager.collection.get(
                                where={"source_file": file.name}
                            )
                            
                            if query_result and len(query_result.get('ids', [])) > 0:
                                chunk_count = len(query_result['ids'])
                                parsed_files.append({
                                    'name': file.name,
                                    'chunk_count': chunk_count,
                                    'file_obj': file
                                })
                            else:
                                unparsed_files.append({
                                    'name': file.name,
                                    'file_obj': file
                                })
                        except Exception as e:
                            st.error(f"æ£€æŸ¥æ–‡ä»¶ {file.name} æ—¶å‡ºé”™ï¼š{e}")
                            unparsed_files.append({
                                'name': file.name,
                                'file_obj': file
                            })
                
                # æ˜¾ç¤ºæ£€æŸ¥ç»“æœ
                if parsed_files:
                    st.warning(f"âš ï¸ å‘ç° {len(parsed_files)} ä¸ªå·²è§£æçš„æ–‡æ¡£ï¼š")
                    for file_info in parsed_files:
                        st.info(f"ğŸ“„ {file_info['name']} - å·²æœ‰ {file_info['chunk_count']} ä¸ªåˆ‡ç‰‡")
                
                if unparsed_files:
                    st.success(f"âœ… å‘ç° {len(unparsed_files)} ä¸ªæœªè§£æçš„æ–‡æ¡£ï¼š")
                    for file_info in unparsed_files:
                        st.info(f"ğŸ“„ {file_info['name']} - æ–°æ–‡æ¡£")
                
                st.markdown("---")
                
                # å¤„ç†é€‰é¡¹
                if parsed_files:
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        if st.button("âœ… è·³è¿‡å·²è§£ææ–‡æ¡£", type="primary"):
                            # æ£€æŸ¥å·²è§£ææ–‡æ¡£çš„åˆ‡ç‰‡æ˜¯å¦éƒ½å·²å…¥åº“
                            with st.spinner("ğŸ” æ£€æŸ¥æ–‡æ¡£åˆ‡ç‰‡å…¥åº“çŠ¶æ€..."):
                                all_chunks_stored = True
                                missing_chunks = []
                                
                                for file_info in parsed_files:
                                    file_name = file_info['name']
                                    try:
                                        # æ£€æŸ¥è¯¥æ–‡ä»¶çš„åˆ‡ç‰‡æ•°é‡
                                        query_result = st.session_state.db_manager.collection.get(
                                            where={"source_file": file_name}
                                        )
                                        stored_count = len(query_result.get('ids', []))
                                        expected_count = file_info['chunk_count']
                                        
                                        if stored_count < expected_count:
                                            all_chunks_stored = False
                                            missing_chunks.append({
                                                'file': file_name,
                                                'stored': stored_count,
                                                'expected': expected_count
                                            })
                                    except Exception as e:
                                        st.error(f"æ£€æŸ¥æ–‡ä»¶ {file_name} æ—¶å‡ºé”™ï¼š{e}")
                                        all_chunks_stored = False
                                
                                if all_chunks_stored:
                                    st.success("âœ… æ‰€æœ‰æ–‡æ¡£åˆ‡ç‰‡å·²å…¥åº“ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼è¯·åˆ‡æ¢åˆ°'æ™ºèƒ½å›¾åƒé—®ç­”'æ ‡ç­¾é¡µå¼€å§‹æé—®")
                                    st.balloons()
                                else:
                                    st.warning(f"âš ï¸ å‘ç° {len(missing_chunks)} ä¸ªæ–‡æ¡£çš„åˆ‡ç‰‡æœªå®Œå…¨å…¥åº“")
                                    for chunk_info in missing_chunks:
                                        st.info(f"ğŸ“„ {chunk_info['file']}: å·²å…¥åº“ {chunk_info['stored']}/{chunk_info['expected']} ä¸ªåˆ‡ç‰‡")
                                    
                                    if st.button("ğŸ“¥ è¡¥å……å…¥åº“ç¼ºå¤±åˆ‡ç‰‡", type="secondary"):
                                        st.info("è¯·é‡æ–°è§£æç›¸å…³æ–‡æ¡£ä»¥è¡¥å……ç¼ºå¤±çš„åˆ‡ç‰‡")
                                        st.session_state.files_to_process = [f['file_obj'] for f in parsed_files if f['name'] in [c['file'] for c in missing_chunks]]
                                        st.session_state.processing_mode = 'reparse'
                                        st.rerun()
                    
                    with col2:
                        if st.button("ğŸ”„ é‡æ–°è§£æå·²å­˜åœ¨çš„æ–‡æ¡£", type="secondary"):
                            st.session_state.files_to_process = [f['file_obj'] for f in parsed_files]
                            st.session_state.processing_mode = 'reparse'
                            st.rerun()
                    
                    with col3:
                        if unparsed_files and st.button("ğŸ“¤ è§£ææ–°æ–‡æ¡£", type="secondary"):
                            st.session_state.files_to_process = [f['file_obj'] for f in unparsed_files]
                            st.session_state.processing_mode = 'new'
                            st.rerun()
                else:
                    # å¦‚æœæ²¡æœ‰å·²è§£ææ–‡æ¡£ï¼Œä½¿ç”¨åŸæ¥çš„ä¸¤åˆ—å¸ƒå±€
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        pass  # ç©ºåˆ—
                    
                    with col2:
                        if unparsed_files and st.button("ğŸ“¤ è§£ææ–°æ–‡æ¡£", type="primary"):
                            st.session_state.files_to_process = [f['file_obj'] for f in unparsed_files]
                            st.session_state.processing_mode = 'new'
                            st.rerun()
                
                # å¦‚æœä¸¤ç§æ–‡æ¡£éƒ½æœ‰ï¼Œæä¾›å…¨éƒ¨å¤„ç†é€‰é¡¹
                if parsed_files and unparsed_files:
                    st.markdown("---")
                    if st.button("ğŸ”„ å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼ˆé‡æ–°è§£æå·²å­˜åœ¨çš„ + è§£ææ–°æ–‡æ¡£ï¼‰", type="primary"):
                        st.session_state.files_to_process = uploaded_files
                        st.session_state.processing_mode = 'all'
                        st.rerun()
            
            # å¤„ç†æ–‡ä»¶è§£æ
            if st.session_state.get('files_to_process') and st.session_state.get('processing_mode'):
                files_to_process = st.session_state.files_to_process
                processing_mode = st.session_state.processing_mode
                
                # æ¸…ç†çŠ¶æ€
                st.session_state.files_to_process = None
                st.session_state.processing_mode = None
                
                # ç¡®å®šå¤„ç†æ¨¡å¼
                if processing_mode == 'reparse':
                    mode_text = "é‡æ–°è§£æ"
                    is_reparse_mode = True
                elif processing_mode == 'new':
                    mode_text = "è§£æ"
                    is_reparse_mode = False
                else:  # 'all'
                     mode_text = "å¤„ç†"
                     is_reparse_mode = False  # æ··åˆæ¨¡å¼ï¼Œåœ¨å¤„ç†æ—¶å•ç‹¬åˆ¤æ–­
                
                # åˆ›å»ºè¿›åº¦å®¹å™¨
                progress_container = st.container()
                
                with progress_container:
                    # æ€»ä½“è¿›åº¦æ¡
                    overall_progress = st.progress(0.0)
                    status_text = st.empty()
                    
                    try:
                        total_files = len(files_to_process)
                        status_text.text(f"å¼€å§‹{mode_text} {total_files} ä¸ªæ–‡ä»¶...")
                        
                        all_duplicate_results = {'duplicates': [], 'new_items': []}
                        
                        for file_idx, uploaded_file in enumerate(files_to_process):
                            # æ›´æ–°æ€»ä½“è¿›åº¦
                            current_progress = file_idx / total_files
                            overall_progress.progress(current_progress)
                            status_text.text(f"æ­£åœ¨{mode_text}æ–‡ä»¶ {file_idx + 1}/{total_files}: {uploaded_file.name}")
                            
                            # å¯¹äºæ··åˆæ¨¡å¼ï¼Œéœ€è¦æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦å·²è§£æè¿‡
                            current_file_is_reparse = is_reparse_mode
                            if processing_mode == 'all':
                                # æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦å·²è§£æè¿‡
                                try:
                                    query_result = st.session_state.db_manager.collection.get(
                                        where={"source_file": uploaded_file.name}
                                    )
                                    current_file_is_reparse = query_result and len(query_result.get('ids', [])) > 0
                                except:
                                    current_file_is_reparse = False
                            
                            # ä½¿ç”¨æ–°çš„å·¥å…·ç±»å¤„ç†æ–‡æ¡£
                            try:
                                # æ­¥éª¤1: ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                                with st.spinner(f"ğŸ“ ä¿å­˜ä¸´æ—¶æ–‡ä»¶: {uploaded_file.name}"):
                                    temp_file_path = f"/tmp/{uploaded_file.name}"
                                    with open(temp_file_path, "wb") as f:
                                        f.write(uploaded_file.getvalue())
                                
                                # æ­¥éª¤2: è§£ææ–‡æ¡£
                                with st.spinner(f"ğŸ” è§£ææ–‡æ¡£å†…å®¹: {uploaded_file.name}"):
                                    chunks = process_document_with_agents(temp_file_path, st.session_state.config)
                                
                                if chunks:
                                    # æ­¥éª¤3: å¤„ç†åˆ‡ç‰‡å’Œæ£€æŸ¥é‡å¤
                                    with st.spinner(f"âœ‚ï¸ å¤„ç†æ–‡æ¡£åˆ‡ç‰‡: å‘ç° {len(chunks)} ä¸ªåˆ‡ç‰‡"):
                                        for chunk in chunks:
                                            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤åˆ‡ç‰‡ï¼ˆåŸºäºå†…å®¹å’Œæ–‡ä»¶åï¼‰
                                            chunk_data = {
                                                'content': chunk.content,
                                                'metadata': {
                                                    'source_file': uploaded_file.name,
                                                    'chunk_type': chunk.chunk_type.value,
                                                    'chunk_id': chunk.chunk_id,
                                                    'page_idx': chunk.page_idx,
                                                    'chunk_idx': chunk.chunk_idx,
                                                    'parent_document': chunk.parent_document,
                                                    **chunk.metadata  # åŒ…å«æ‰€æœ‰åŸå§‹å…ƒæ•°æ®
                                                }
                                            }
                                            
                                            # ä¸ºå›¾åƒå’Œè¡¨æ ¼ç±»åˆ‡ç‰‡æ·»åŠ Base64æ•°æ®
                                            if chunk.chunk_type.value in ['image', 'table'] and chunk.content_path:
                                                try:
                                                    import base64
                                                    with open(chunk.content_path, 'rb') as f:
                                                        image_data = base64.b64encode(f.read()).decode('utf-8')
                                                        chunk_data['original_content'] = image_data
                                                except Exception as e:
                                                    print(f"è¯»å–å›¾ç‰‡æ–‡ä»¶å¤±è´¥ {chunk.content_path}: {e}")
                                                    # å¦‚æœè¯»å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç›¸å¯¹è·¯å¾„
                                                    try:
                                                        image_path = chunk.metadata.get('image_path') or chunk.metadata.get('table_image_path')
                                                        if image_path:
                                                            # æ ¹æ®source_fileåŠ¨æ€æ„å»ºbase_dirï¼Œå¹¶å°è¯•ä¸åŒçš„å¤„ç†æ–¹æ³•
                                                            source_file = chunk.source_file or uploaded_file.name.split('.')[0]
                                                            config = st.session_state.config
                                                            
                                                            # å°è¯•ä¸åŒçš„å¤„ç†æ–¹æ³•ï¼šauto, ocr
                                                            for method in ['auto', 'ocr']:
                                                                base_dir = os.path.join(config.output_dir, config.cls_dir, source_file, source_file, method)
                                                                full_path = os.path.join(base_dir, image_path)
                                                                if os.path.exists(full_path):
                                                                    with open(full_path, 'rb') as f:
                                                                        image_data = base64.b64encode(f.read()).decode('utf-8')
                                                                        chunk_data['original_content'] = image_data
                                                                    break
                                                            else:
                                                                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè®°å½•é”™è¯¯
                                                                print(f"åœ¨æ‰€æœ‰å¤„ç†æ–¹æ³•ç›®å½•ä¸­éƒ½æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶: {image_path}")
                                                    except Exception as e2:
                                                        print(f"ä½¿ç”¨ç›¸å¯¹è·¯å¾„è¯»å–å›¾ç‰‡æ–‡ä»¶ä¹Ÿå¤±è´¥: {e2}")
                                                        print(f"å°è¯•çš„è·¯å¾„: {full_path if 'full_path' in locals() else 'è·¯å¾„æ„å»ºå¤±è´¥'}")
                                            
                                            # å¦‚æœæ˜¯é‡æ–°è§£ææ¨¡å¼æˆ–å½“å‰æ–‡ä»¶å·²è§£æè¿‡ï¼Œæ‰€æœ‰åˆ‡ç‰‡éƒ½è§†ä¸ºéœ€è¦è¦†ç›–çš„é‡å¤é¡¹
                                            if current_file_is_reparse:
                                                # ä¸ºé‡æ–°è§£æçš„åˆ‡ç‰‡æ·»åŠ existing_docå­—æ®µ
                                                try:
                                                    existing = st.session_state.db_manager.collection.get(
                                                        where={
                                                            "$and": [
                                                                {"source_file": uploaded_file.name},
                                                                {"chunk_id": chunk.chunk_id}
                                                            ]
                                                        }
                                                    )
                                                    if existing and len(existing.get('ids', [])) > 0:
                                                        # æ„é€ existing_docç»“æ„
                                                        existing_doc = {
                                                            'id': existing['ids'][0],
                                                            'document': existing['documents'][0],
                                                            'metadata': existing['metadatas'][0]
                                                        }
                                                        chunk_data['existing_doc'] = existing_doc
                                                except:
                                                    # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„existing_doc
                                                    chunk_data['existing_doc'] = {
                                                        'id': 'unknown',
                                                        'document': 'æ— æ³•è·å–å·²å­˜åœ¨æ–‡æ¡£ä¿¡æ¯',
                                                        'metadata': {}
                                                    }
                                                all_duplicate_results['duplicates'].append(chunk_data)
                                            else:
                                                # æ­£å¸¸æ¨¡å¼ä¸‹æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨
                                                try:
                                                    existing = st.session_state.db_manager.collection.get(
                                                        where={
                                                            "$and": [
                                                                {"source_file": uploaded_file.name},
                                                                {"chunk_id": chunk.chunk_id}
                                                            ]
                                                        }
                                                    )
                                                    if existing and len(existing.get('ids', [])) > 0:
                                                        # æ„é€ existing_docç»“æ„
                                                        existing_doc = {
                                                            'id': existing['ids'][0],
                                                            'document': existing['documents'][0],
                                                            'metadata': existing['metadatas'][0]
                                                        }
                                                        chunk_data['existing_doc'] = existing_doc
                                                        all_duplicate_results['duplicates'].append(chunk_data)
                                                    else:
                                                        all_duplicate_results['new_items'].append(chunk_data)
                                                except:
                                                    # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œé»˜è®¤ä¸ºæ–°é¡¹ç›®
                                                    all_duplicate_results['new_items'].append(chunk_data)
                                    
                                    file_mode = "é‡æ–°è§£æ" if current_file_is_reparse else "è§£æ"
                                    st.success(f"âœ… {uploaded_file.name} {file_mode}å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ‡ç‰‡")
                                else:
                                    st.warning(f"âš ï¸ {uploaded_file.name} æœªèƒ½ç”Ÿæˆæœ‰æ•ˆåˆ‡ç‰‡")
                                
                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                if os.path.exists(temp_file_path):
                                    os.remove(temp_file_path)
                                    
                            except Exception as e:
                                st.error(f"âŒ {mode_text}æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™ï¼š{e}")
                                continue
                        
                        # å®Œæˆå¤„ç†
                        overall_progress.progress(1.0)
                        status_text.text(f"æ–‡ä»¶{mode_text}å®Œæˆï¼")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤é¡¹æˆ–æ–°é¡¹ç›®
                        has_duplicates = len(all_duplicate_results['duplicates']) > 0
                        has_new_items = len(all_duplicate_results['new_items']) > 0
                        
                        if has_duplicates or has_new_items:
                            duplicate_count = len(all_duplicate_results['duplicates'])
                            new_count = len(all_duplicate_results['new_items'])
                            
                            if processing_mode == 'reparse':
                                st.info(f"ğŸ“Š é‡æ–°è§£æç»“æœï¼šå‘ç° {duplicate_count} ä¸ªåˆ‡ç‰‡å°†è¢«è¦†ç›–")
                            elif processing_mode == 'new':
                                st.info(f"ğŸ“Š è§£æç»“æœï¼šå‘ç° {new_count} ä¸ªæ–°åˆ‡ç‰‡")
                            else:  # 'all'
                                st.info(f"ğŸ“Š å¤„ç†ç»“æœï¼šå‘ç° {duplicate_count} ä¸ªé‡å¤åˆ‡ç‰‡ï¼Œ{new_count} ä¸ªæ–°åˆ‡ç‰‡")
                            
                            # æ˜¾ç¤ºåˆ‡ç‰‡å…¥åº“ç¡®è®¤ç•Œé¢
                            st.session_state.duplicate_check_results = all_duplicate_results
                            st.session_state.show_duplicate_confirmation = True
                            st.rerun()
                        else:
                            st.warning("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹")
                            
                    except Exception as e:
                        st.error(f"âŒ {mode_text}æ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}")
                    finally:
                        # æ¸…ç†è¿›åº¦æ˜¾ç¤º
                        overall_progress.empty()
                        status_text.empty()
    
    with tab2:
        st.header("ğŸ” æœç´¢æŸ¥è¯¢")
        
        # Check if database has documents
        if st.session_state.db_manager:
            info = st.session_state.db_manager.get_collection_info()
            if info.get('count', 0) == 0:
                st.info("æ•°æ®åº“ä¸­æš‚æ— æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ å›¾ç‰‡æˆ–PDFæ–‡ä»¶")
                return
        
        # æ™ºèƒ½é—®ç­”ç•Œé¢ï¼ˆåˆå¹¶äº†å›¾åƒæ£€ç´¢å’Œé—®ç­”åŠŸèƒ½ï¼‰
            st.subheader("æ™ºèƒ½é—®ç­”")
            st.markdown("ğŸ’¡ **ä½¿ç”¨è¯´æ˜ï¼š** è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³èµ„æ–™å¹¶åŸºäºå†…å®¹ç”Ÿæˆå›ç­”")
        
            # é—®ç­”è¾“å…¥åŒºåŸŸ
            user_question = st.text_area(
                "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿç¥ç»ç½‘ç»œçš„ç»“æ„æ˜¯æ€æ ·çš„ï¼Ÿè¿™ä¸ªæ•°æ®å¯è§†åŒ–å›¾è¡¨è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ",
                help="è¾“å…¥æ‚¨æƒ³äº†è§£çš„é—®é¢˜ï¼Œç³»ç»Ÿä¼šæ£€ç´¢ç›¸å…³èµ„æ–™å¹¶ç”Ÿæˆå›ç­”",
                height=120
            )
            
            if user_question and user_question.strip():
                if st.button("ğŸ” æœç´¢å¹¶é—®ç­”", type="primary"):
                    with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³èµ„æ–™..."):
                        try:
                            # è·å–é…ç½®
                            config = st.session_state.config
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨æ‰©å†™
                            original_question = user_question
                            expanded_question = user_question
                            auto_expanded = False
                            
                            if config.enable_auto_expand and len(user_question) < config.auto_expand_min_length:
                                st.info(f"ğŸ”„ æ£€æµ‹åˆ°æŸ¥è¯¢å­—æ•°å°‘äº{config.auto_expand_min_length}å­—ï¼Œæ­£åœ¨è‡ªåŠ¨æ‰©å†™...")
                                try:
                                    # å¯¼å…¥æŸ¥è¯¢æ‰©å†™ä»£ç†
                                    from agent.query_agent import QueryExpansionAgent
                                    query_agent = QueryExpansionAgent()
                                    expanded_question = query_agent.auto_expand_query_sync(user_question, config.auto_expand_min_length)
                                    auto_expanded = True
                                    
                                    # æ˜¾ç¤ºæ‰©å†™ç»“æœ
                                    with st.expander("ğŸ“ æŸ¥è¯¢è‡ªåŠ¨æ‰©å†™ç»“æœ", expanded=True):
                                        st.markdown(f"**åŸå§‹é—®é¢˜ï¼š** {original_question}")
                                        st.markdown(f"**è‡ªåŠ¨æ‰©å†™ï¼š** {expanded_question}")
                                        st.info("ğŸ’¡ æ‰©å†™åçš„æŸ¥è¯¢å°†ç”¨äºæ£€ç´¢ï¼Œä½†AIå›ç­”æ—¶ä¼šæ›´å…³æ³¨åŸå§‹é—®é¢˜")
                                        
                                except Exception as e:
                                    st.warning(f"è‡ªåŠ¨æ‰©å†™å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼š{e}")
                                    expanded_question = user_question
                            
                            # ç¬¬ä¸€æ­¥ï¼šåŸºäºé—®é¢˜æ£€ç´¢ç›¸å…³èµ„æ–™
                            threshold = st.session_state.similarity_threshold if st.session_state.use_similarity_threshold else None
                            
                            search_results = search_similar_images(
                                expanded_question, 
                                st.session_state.db_manager, 
                                st.session_state.num_results,
                                threshold
                            )
                            
                            if not search_results:
                                if st.session_state.use_similarity_threshold:
                                    st.warning(f"æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼åº¦é«˜äº {st.session_state.similarity_threshold:.2f} çš„ç›¸å…³èµ„æ–™ï¼Œè¯·å°è¯•é™ä½é˜ˆå€¼æˆ–ä½¿ç”¨å…¶ä»–å…³é”®è¯")
                                else:
                                    st.warning("æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„èµ„æ–™ï¼Œè¯·å°è¯•å…¶ä»–é—®é¢˜")
                                return
                            
                            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
                            st.subheader(f"ğŸ“š æ£€ç´¢åˆ° {len(search_results)} ä¸ªç›¸å…³èµ„æ–™")
                            if st.session_state.use_similarity_threshold:
                                st.info(f"å·²å¯ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ (â‰¥{st.session_state.similarity_threshold:.2f})")
                            
                            # æ˜¾ç¤ºæ¯ä¸ªæ£€ç´¢ç»“æœçš„è¯¦ç»†å†…å®¹
                            st.markdown("### ğŸ” æ£€ç´¢ç»“æœ")
                            for i, result in enumerate(search_results):
                                display_chunk_content(result, i)
                            
                            # ç¬¬äºŒæ­¥ï¼šåŸºäºé—®é¢˜å’Œæ£€ç´¢åˆ°çš„èµ„æ–™ç”Ÿæˆå›ç­”
                            st.markdown("### ğŸ¤– AI åˆ†æä¸å›ç­”")
                            with st.spinner("AIæ­£åœ¨åˆ†æèµ„æ–™å¹¶ç”Ÿæˆå›ç­”..."):
                                import asyncio
                                
                                async def get_answer():
                                    # æ„å»ºä¼ é€’ç»™ç­”æ¡ˆä»£ç†çš„æŸ¥è¯¢ä¿¡æ¯
                                    if auto_expanded:
                                        query_for_answer = f"è‡ªåŠ¨æ‰©å†™: {expanded_question}\n\nåŸå§‹é—®é¢˜: {original_question}"
                                    else:
                                        query_for_answer = original_question
                                    
                                    return await st.session_state.answer_agent.answer_question(
                                        query_for_answer, 
                                        search_results
                                    )
                                
                                # åœ¨Streamlitä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
                                answer_result = asyncio.run(get_answer())
                                
                                if answer_result.get("error"):
                                    st.error(f"é—®ç­”è¿‡ç¨‹ä¸­å‡ºé”™ï¼š{answer_result['error']}")
                                else:
                                    # æ˜¾ç¤ºAIå›ç­”
                                    st.markdown("#### ğŸ’¬ AI å›ç­”")
                                    st.write(answer_result.get("answer", "æ— æ³•ç”Ÿæˆå›ç­”"))
                                    
                                    # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
                                    usage = answer_result.get("usage_metadata", {})
                                    if usage:
                                        with st.expander("ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡", expanded=False):
                                            col1, col2, col3 = st.columns(3)
                                            with col1:
                                                st.metric("è¾“å…¥Token", usage.get("input_tokens", 0))
                                            with col2:
                                                st.metric("è¾“å‡ºToken", usage.get("output_tokens", 0))
                                            with col3:
                                                st.metric("æ€»Token", usage.get("total_tokens", 0))
                                
                        except Exception as e:
                            st.error(f"æ‰§è¡Œé—®ç­”æ—¶å‡ºé”™ï¼š{e}")
            
            # æ·»åŠ ä½¿ç”¨æç¤ºå’Œç¤ºä¾‹
            with st.expander("ğŸ’¡ ä½¿ç”¨æç¤ºå’Œç¤ºä¾‹", expanded=False):
                st.markdown("""
                ### ğŸ’¡ å¦‚ä½•æ›´å¥½åœ°ä½¿ç”¨æ™ºèƒ½é—®ç­”åŠŸèƒ½ï¼š
                
                **å·¥ä½œæµç¨‹ï¼š**
                1. è¾“å…¥æ‚¨çš„é—®é¢˜
                2. ç³»ç»Ÿè‡ªåŠ¨æ£€ç´¢ç›¸å…³å›¾ç‰‡
                3. AIåŸºäºé—®é¢˜å’Œå›¾ç‰‡ç”Ÿæˆå›ç­”
                
                **æé—®æŠ€å·§ï¼š**
                - æå‡ºå…·ä½“ã€æ˜ç¡®çš„é—®é¢˜
                - å¯ä»¥è¯¢é—®æ¦‚å¿µã€åŸç†ã€æ•°æ®åˆ†æç­‰
                - æ”¯æŒä¸­è‹±æ–‡é—®é¢˜
                
                ### ğŸ“ ç¤ºä¾‹é—®é¢˜ï¼š
                
                **æ¦‚å¿µè§£é‡Šç±»ï¼š**
                - ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ
                - ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ
                - å·ç§¯ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
                
                **æ•°æ®åˆ†æç±»ï¼š**
                - è¿™ä¸ªå›¾è¡¨æ˜¾ç¤ºäº†ä»€ä¹ˆè¶‹åŠ¿ï¼Ÿ
                - æ•°æ®å¯è§†åŒ–å›¾ä¸­çš„ä¸»è¦ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ
                - è¿™äº›ç»Ÿè®¡æ•°æ®è¯´æ˜äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
                
                **æŠ€æœ¯æ¶æ„ç±»ï¼š**
                - è¿™ä¸ªç³»ç»Ÿæ¶æ„å›¾å±•ç¤ºäº†ä»€ä¹ˆï¼Ÿ
                - æŠ€æœ¯æµç¨‹å›¾ä¸­çš„å…³é”®æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ
                - è¿™ä¸ªç®—æ³•çš„å®ç°åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ
                """)
    
    with tab3:
        display_knowledge_base_management()
    
    # Footer
    st.markdown("---")
    with st.expander("â„¹ï¸ å…³äºç³»ç»Ÿ"):
        st.markdown("""
        ### Vision RAG with Qwen
        
        æœ¬ç³»ç»ŸåŸºäºä»¥ä¸‹æŠ€æœ¯æ„å»ºï¼š
        
        - **é€šä¹‰åƒé—®å¤šæ¨¡æ€åµŒå…¥**: ç”¨äºå›¾ç‰‡å’Œæ–‡æœ¬çš„å‘é‡åŒ–
        - **é€šä¹‰åƒé—®è§†è§‰è¯­è¨€æ¨¡å‹**: ç”¨äºå›¾åƒç†è§£å’Œé—®ç­”
        - **ChromaDB**: é«˜æ€§èƒ½å‘é‡æ•°æ®åº“ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢
        - **Streamlit**: ç”¨æˆ·ç•Œé¢æ¡†æ¶
        - **PyMuPDF**: PDFæ–‡æ¡£å¤„ç†
        - **LangGraph**: å·¥ä½œæµç¼–æ’æ¡†æ¶
        
        ### åŠŸèƒ½ç‰¹ç‚¹
        
        - æ”¯æŒå›¾ç‰‡å’ŒPDFæ–‡æ¡£ä¸Šä¼ 
        - è‡ªåŠ¨å‘é‡åŒ–å’Œå­˜å‚¨
        - åŸºäºæ–‡æœ¬æŸ¥è¯¢çš„ç›¸ä¼¼å›¾ç‰‡æ£€ç´¢
        - æ™ºèƒ½å›¾åƒé—®ç­”åŠŸèƒ½
        - ç›´è§‚çš„æœç´¢ç»“æœå±•ç¤º
        - å¯é…ç½®çš„æœç´¢å‚æ•°å’Œç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        - çµæ´»çš„é…ç½®ç®¡ç†ç³»ç»Ÿ
        - é«˜çº§æ–‡æ¡£è§£æé€‰é¡¹
        - Tokenä½¿ç”¨ç»Ÿè®¡
        
        ### ä½¿ç”¨æ–¹æ³•
        
        1. **ä¸Šä¼ æ–‡æ¡£**: åœ¨"ä¸Šä¼ æ–‡æ¡£"æ ‡ç­¾é¡µä¸­ä¸Šä¼ å›¾ç‰‡æˆ–PDFæ–‡ä»¶
        2. **å›¾åƒæœç´¢**: åœ¨"æœç´¢æŸ¥è¯¢"â†’"å›¾åƒæœç´¢"ä¸­è¾“å…¥æŸ¥è¯¢æ–‡æœ¬ï¼Œæ£€ç´¢ç›¸ä¼¼å›¾åƒ
        3. **æ™ºèƒ½é—®ç­”**: åœ¨"æœç´¢æŸ¥è¯¢"â†’"æ™ºèƒ½é—®ç­”"ä¸­åŸºäºæ£€ç´¢åˆ°çš„å›¾åƒè¿›è¡Œæé—®
        4. **æŸ¥çœ‹ç»“æœ**: ç³»ç»Ÿä¼šæ˜¾ç¤ºè¯¦ç»†çš„å›ç­”å’Œä½¿ç”¨ç»Ÿè®¡
        
        ### æ–°å¢åŠŸèƒ½ï¼šæ™ºèƒ½é—®ç­”
        
        - åŸºäºæ£€ç´¢åˆ°çš„å›¾åƒå†…å®¹è¿›è¡Œæ™ºèƒ½é—®ç­”
        - æ”¯æŒå¤šå›¾åƒç»¼åˆåˆ†æ
        - å‡†ç¡®è¯†åˆ«å›¾åƒä¸­çš„æ–‡å­—å’Œå›¾è¡¨
        - æä¾›è¯¦ç»†çš„æŠ€æœ¯è§£é‡Šå’Œå†…å®¹åˆ†æ
        - å®æ—¶æ˜¾ç¤ºTokenä½¿ç”¨æƒ…å†µ
        
        ### é…ç½®ç®¡ç†
        
        - ç»Ÿä¸€çš„é…ç½®ç±»ç®¡ç†æ‰€æœ‰å‚æ•°
        - æ”¯æŒé«˜çº§æ–‡æ¡£è§£æé…ç½®
        - çµæ´»çš„åˆ‡ç‰‡ç­–ç•¥è®¾ç½®
        - å¯é…ç½®çš„æ£€ç´¢å‚æ•°
        - ç•Œé¢æ˜¾ç¤ºé€‰é¡¹æ§åˆ¶
        """)

if __name__ == "__main__":
    main()